{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c01554-710d-43ee-bba3-33c29f1b34d0",
   "metadata": {},
   "source": [
    "# STA 6543: Predictive Modeing\n",
    "# Project \n",
    "\n",
    "Name:\n",
    "\n",
    "    Angel Manuel Perez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4362c19-8d37-48fb-844d-2e48d4bad9fc",
   "metadata": {},
   "source": [
    "# Background\n",
    "## A national veterans’ organization wishes to develop a predictive model to improve the cost-effectiveness of their direct marketing campaign. The organization, with its in-house database of over 13 million donors, is one of the largest direct-mail fundraisers in the United States. According to their recent mailing records, the overall response rate is 5.1%. Out of those who responded (donated), the average donation is `$13.00.` Each mailing, which includes a gift of personalized address labels and assortments of cards and envelopes, costs $0.68 to produce and send. Using these facts, we take a sample of this dataset to develop a classification model that can effectively capture donors so that the expected net profit is maximized. Weighted sampling was used, under-representing the non-responders so that the sample has equal numbers of donors and non-donors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c9c86-3b5d-454e-89f2-bf456b968430",
   "metadata": {},
   "source": [
    "# Data\n",
    "## The fundraising training file Download fundraising training file contains 3,000 records with approximately 50% donors (target = Donor) and 50% non-donors (target = No Donor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a617597-8124-4eb4-91cc-3e28071edf62",
   "metadata": {},
   "source": [
    "# Step 1: \n",
    "## Partitioning. You might think about how to estimate the out of sample error. Either partition the dataset into 80% training and 20% validation or use cross validation (set the seed to 12345)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b38aea-a32b-400b-8040-21f5723044ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipconvert2</th>\n",
       "      <th>zipconvert3</th>\n",
       "      <th>zipconvert4</th>\n",
       "      <th>zipconvert5</th>\n",
       "      <th>homeowner</th>\n",
       "      <th>num_child</th>\n",
       "      <th>income</th>\n",
       "      <th>female</th>\n",
       "      <th>wealth</th>\n",
       "      <th>home_value</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_fam_inc</th>\n",
       "      <th>pct_lt15k</th>\n",
       "      <th>num_prom</th>\n",
       "      <th>lifetime_gifts</th>\n",
       "      <th>largest_gift</th>\n",
       "      <th>last_gift</th>\n",
       "      <th>months_since_donate</th>\n",
       "      <th>time_lag</th>\n",
       "      <th>avg_gift</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>7</td>\n",
       "      <td>698</td>\n",
       "      <td>...</td>\n",
       "      <td>463</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>94.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>Donor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>8</td>\n",
       "      <td>828</td>\n",
       "      <td>...</td>\n",
       "      <td>376</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>Donor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>1471</td>\n",
       "      <td>...</td>\n",
       "      <td>546</td>\n",
       "      <td>4</td>\n",
       "      <td>94</td>\n",
       "      <td>177.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>7.080000</td>\n",
       "      <td>No Donor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>8</td>\n",
       "      <td>547</td>\n",
       "      <td>...</td>\n",
       "      <td>432</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>No Donor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>8</td>\n",
       "      <td>482</td>\n",
       "      <td>...</td>\n",
       "      <td>275</td>\n",
       "      <td>28</td>\n",
       "      <td>38</td>\n",
       "      <td>73.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>Donor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  zipconvert2 zipconvert3 zipconvert4 zipconvert5 homeowner  num_child  \\\n",
       "0         Yes          No          No          No       Yes          1   \n",
       "1          No          No          No         Yes        No          2   \n",
       "2          No          No          No         Yes       Yes          1   \n",
       "3          No         Yes          No          No       Yes          1   \n",
       "4          No         Yes          No          No       Yes          1   \n",
       "\n",
       "   income female  wealth  home_value  ...  avg_fam_inc  pct_lt15k  num_prom  \\\n",
       "0       1     No       7         698  ...          463          4        46   \n",
       "1       5    Yes       8         828  ...          376         13        32   \n",
       "2       3     No       4        1471  ...          546          4        94   \n",
       "3       4     No       8         547  ...          432          7        20   \n",
       "4       4    Yes       8         482  ...          275         28        38   \n",
       "\n",
       "   lifetime_gifts  largest_gift  last_gift  months_since_donate  time_lag  \\\n",
       "0            94.0          12.0       12.0                   34         6   \n",
       "1            30.0          10.0        5.0                   29         7   \n",
       "2           177.0          10.0        8.0                   30         3   \n",
       "3            23.0          11.0       11.0                   30         6   \n",
       "4            73.0          10.0       10.0                   31         3   \n",
       "\n",
       "   avg_gift    target  \n",
       "0  9.400000     Donor  \n",
       "1  4.285714     Donor  \n",
       "2  7.080000  No Donor  \n",
       "3  7.666667  No Donor  \n",
       "4  7.300000     Donor  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(12345)\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"data/fundraising.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a6485d-45aa-4e1d-b7d8-f4d286170602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target variable\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "# Partitioning into 80% training and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d5862-8d09-4932-a31c-a6b79f8d0eb5",
   "metadata": {},
   "source": [
    "# Step 2: \n",
    "## Model Building. Follow the following steps to build, evaluate, and choose a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88dc61f-652d-42fd-8889-149d9bac2e8c",
   "metadata": {},
   "source": [
    "### 1. Exploratory data analysis. Examine the predictors and evaluate their association with the response variable. Which might be good candidate predictors? Are any collinear with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb4b7b-a5ae-4950-90ec-529f3621229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of predictors\n",
    "numeric_predictors = ['num_child', 'income', 'wealth', 'home_value', 'med_fam_inc', 'avg_fam_inc', 'pct_lt15k',\n",
    "                      'num_prom', 'lifetime_gifts', 'largest_gift', 'last_gift', 'months_since_donate', 'time_lag', 'avg_gift']\n",
    "categorical_predictors = ['zipconvert2', 'zipconvert3', 'zipconvert4', 'zipconvert5', 'homeowner', 'female']\n",
    "\n",
    "# Arrange plots in rows of 3 or 4\n",
    "def plot_in_rows(predictors, ncols=3):\n",
    "    nrows = (len(predictors) + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5 * nrows))\n",
    "    axes = axes.flatten()\n",
    "    for i, predictor in enumerate(predictors):\n",
    "        ax = axes[i]\n",
    "        if predictor in numeric_predictors:\n",
    "            sns.histplot(data[predictor], kde=True, ax=ax)\n",
    "            ax.set_xlabel(predictor)\n",
    "        else:\n",
    "            sns.countplot(x=predictor, hue='target', data=data, ax=ax)\n",
    "            ax.legend(title='Target', loc='upper right', labels=['Non-donor', 'Donor'])\n",
    "            ax.set_ylabel('Frequency')\n",
    "        ax.set_title(f'Distribution of {predictor}')\n",
    "\n",
    "# Plot numeric predictors\n",
    "plot_in_rows(numeric_predictors)\n",
    "\n",
    "# Plot categorical predictors\n",
    "plot_in_rows(categorical_predictors, ncols=4)\n",
    "\n",
    "# Correlation analysis\n",
    "correlation_matrix = data[numeric_predictors].corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numeric Predictors')\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix\n",
    "correlation_matrix = data[numeric_predictors].corr()\n",
    "\n",
    "# Correlation Matrix DataFrame\n",
    "correlation_matrix_df = pd.DataFrame(correlation_matrix, columns=numeric_predictors, index=numeric_predictors)\n",
    "print(\"Correlation Matrix:\")\n",
    "display(correlation_matrix_df)\n",
    "\n",
    "# Identify highly correlated pairs\n",
    "highly_correlated_pairs = []\n",
    "for i in range(len(numeric_predictors)):\n",
    "    for j in range(i+1, len(numeric_predictors)):\n",
    "        predictor1 = numeric_predictors[i]\n",
    "        predictor2 = numeric_predictors[j]\n",
    "        correlation_coefficient = correlation_matrix.loc[predictor1, predictor2]\n",
    "        if abs(correlation_coefficient) > 0.7:\n",
    "            highly_correlated_pairs.append((predictor1, predictor2, correlation_coefficient))\n",
    "\n",
    "# Highly Correlated Pairs DataFrame\n",
    "if highly_correlated_pairs:\n",
    "    highly_correlated_pairs_df = pd.DataFrame(highly_correlated_pairs, columns=[\"Predictor 1\", \"Predictor 2\", \"Correlation Coefficient\"])\n",
    "    print(\"\\nHighly Correlated Pairs (correlation coefficient > 0.7):\")\n",
    "    print(highly_correlated_pairs_df)\n",
    "else:\n",
    "    print(\"\\nNo highly correlated pairs found (correlation coefficient > 0.7).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17b343-b64a-4045-b69f-7b63b17ee33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of predictors\n",
    "numeric_predictors = ['num_child', 'income', 'wealth', 'home_value', 'med_fam_inc', 'avg_fam_inc', 'pct_lt15k',\n",
    "                      'num_prom', 'lifetime_gifts', 'largest_gift', 'last_gift', 'months_since_donate', 'time_lag', 'avg_gift']\n",
    "categorical_predictors = ['zipconvert2', 'zipconvert3', 'zipconvert4', 'zipconvert5', 'homeowner', 'female']\n",
    "\n",
    "# Arrange plots in rows of 3 or 4\n",
    "def plot_in_rows(predictors, ncols=3):\n",
    "    nrows = (len(predictors) + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5 * nrows))\n",
    "    axes = axes.flatten()\n",
    "    for i, predictor in enumerate(predictors):\n",
    "        ax = axes[i]\n",
    "        if predictor in numeric_predictors:\n",
    "            sns.histplot(data[predictor], kde=True, ax=ax)\n",
    "            ax.set_xlabel(predictor)\n",
    "        else:\n",
    "            sns.countplot(x=predictor, hue='target', data=data, ax=ax)\n",
    "            ax.legend(title='Target', loc='upper right', labels=['Non-donor', 'Donor'])\n",
    "            ax.set_ylabel('Frequency')\n",
    "        ax.set_title(f'Distribution of {predictor}')\n",
    "\n",
    "# Plot numeric predictors\n",
    "plot_in_rows(numeric_predictors)\n",
    "\n",
    "# Plot categorical predictors\n",
    "plot_in_rows(categorical_predictors, ncols=4)\n",
    "\n",
    "# Correlation analysis\n",
    "correlation_matrix = data[numeric_predictors].corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numeric Predictors')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot for numeric predictors\n",
    "sns.pairplot(data=data, vars=numeric_predictors, hue='target', diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Numeric Predictors', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for categorical predictors\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "for i, predictor in enumerate(categorical_predictors):\n",
    "    sns.boxplot(x=predictor, y='income', data=data, ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot of Income by {predictor}')\n",
    "    axes[i].set_ylabel('Income')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated pairs\n",
    "highly_correlated_pairs = []\n",
    "for i in range(len(numeric_predictors)):\n",
    "    for j in range(i+1, len(numeric_predictors)):\n",
    "        predictor1 = numeric_predictors[i]\n",
    "        predictor2 = numeric_predictors[j]\n",
    "        correlation_coefficient = correlation_matrix.loc[predictor1, predictor2]\n",
    "        if abs(correlation_coefficient) > 0.7:\n",
    "            highly_correlated_pairs.append((predictor1, predictor2, correlation_coefficient))\n",
    "\n",
    "# Highly Correlated Pairs DataFrame\n",
    "if highly_correlated_pairs:\n",
    "    highly_correlated_pairs_df = pd.DataFrame(highly_correlated_pairs, columns=[\"Predictor 1\", \"Predictor 2\", \"Correlation Coefficient\"])\n",
    "    print(\"\\nHighly Correlated Pairs (correlation coefficient > 0.7):\")\n",
    "    print(highly_correlated_pairs_df)\n",
    "else:\n",
    "    print(\"\\nNo highly correlated pairs found (correlation coefficient > 0.7).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eecd5c4-07b1-4b3c-b0ee-6596101c721e",
   "metadata": {},
   "source": [
    "### 2. Select classification tool and parameters. Run at least two classification models of your choosing. Describe the two models that you chose, with sufficient detail (method, parameters, variables, etc.) so that it can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb51fd4b-1f15-4eb9-940a-9c594082d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, subsample=1.0)\n",
    "\n",
    "# Convert categorical variables to one-hot encoding\n",
    "X_train_encoded = pd.get_dummies(X_train)\n",
    "X_val_encoded = pd.get_dummies(X_val)\n",
    "\n",
    "# Initialize and train the models\n",
    "rf_classifier.fit(X_train_encoded, y_train)\n",
    "gb_classifier.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_predictions = rf_classifier.predict(X_val_encoded)\n",
    "gb_predictions = gb_classifier.predict(X_val_encoded)\n",
    "\n",
    "\n",
    "# Define the evaluate_model function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    # Convert string labels to binary numeric values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true_binary = label_encoder.fit_transform(y_true)\n",
    "    y_pred_binary = label_encoder.transform(y_pred)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "    precision = precision_score(y_true_binary, y_pred_binary)\n",
    "    recall = recall_score(y_true_binary, y_pred_binary)\n",
    "    f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_true_binary, y_pred_binary)\n",
    "    \n",
    "    # Return a dictionary containing the evaluation metrics\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1,\n",
    "        'ROC AUC': roc_auc\n",
    "    }\n",
    "\n",
    "# Evaluate Random Forest Classifier\n",
    "rf_metrics = evaluate_model(y_val, rf_predictions, \"Random Forest Classifier\")\n",
    "\n",
    "# Evaluate Gradient Boosting Classifier\n",
    "gb_metrics = evaluate_model(y_val, gb_predictions, \"Gradient Boosting Classifier\")\n",
    "\n",
    "# Create a DataFrame from the evaluation metrics\n",
    "evaluation_results_2 = pd.DataFrame([rf_metrics, gb_metrics])\n",
    "\n",
    "# Display the DataFrame\n",
    "display(evaluation_results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dae056-c8a1-497b-82e9-0f76bdbe680b",
   "metadata": {},
   "source": [
    "### 3. Classification under asymmetric response and cost. Comment on the reasoning behind using weighted sampling to produce a training set with equal numbers of donors and non-donors? Why not use a simple random sample from the original dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fe177-fd12-4158-b910-2b642c67ffbf",
   "metadata": {},
   "source": [
    "Using weighted sampling to produce a training set with equal numbers of donors and non-donors is a strategy often employed in scenarios where the response variable (in this case, donor or non-donor) is imbalanced, and there is a significant cost associated with misclassifying certain classes. In the context of the fundraising campaign for the veterans' organization, there are several reasons why weighted sampling might be preferred over simple random sampling:\n",
    "\n",
    "1. **Imbalanced Response Variable:** The response variable, which indicates whether an individual is a donor or non-donor, is often imbalanced in fundraising datasets. In this case, only around 5.1% of individuals are donors, while the majority are non-donors. When training a classification model on imbalanced data, the model might become biased towards the majority class (non-donors) and perform poorly in predicting the minority class (donors).\n",
    "\n",
    "2. **Cost Implications:** Misclassifying donors as non-donors (false negatives) can be more costly than misclassifying non-donors as donors (false positives) in the context of a fundraising campaign. Donors are the primary source of revenue for the organization, and failing to identify potential donors means missing out on valuable contributions. Therefore, it is crucial to ensure that the model can effectively capture donor behavior.\n",
    "\n",
    "3. **Model Performance:** By using weighted sampling to balance the classes in the training set, the model is trained on a more representative dataset. This helps mitigate the bias towards the majority class and allows the model to learn the patterns and characteristics of both donors and non-donors more effectively. As a result, the model is likely to achieve better performance, particularly in terms of recall and overall predictive accuracy.\n",
    "\n",
    "In contrast, using a simple random sample from the original dataset might result in a training set that is heavily skewed towards non-donors, leading to suboptimal model performance, especially in identifying donors. Weighted sampling addresses this issue by ensuring that the training set contains an equal number of donors and non-donors, thereby improving the model's ability to capture both classes effectively.\n",
    "\n",
    "Overall, weighted sampling is a prudent approach in scenarios where the response variable is imbalanced, and there are asymmetric costs associated with misclassification. It helps improve model performance and ensures that the model is better equipped to identify potential donors, ultimately maximizing the effectiveness of the fundraising campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8922b-70f3-4254-bb57-97c5355ddd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"data/fundraising.csv\")\n",
    "\n",
    "# Separate donors and non-donors\n",
    "donors = data[data['target'] == 'Donor']\n",
    "non_donors = data[data['target'] == 'No Donor']\n",
    "\n",
    "# Determine the number of samples to select for each class (minimum of donors and non-donors)\n",
    "num_samples = min(len(donors), len(non_donors))\n",
    "\n",
    "# Sample an equal number of donors and non-donors\n",
    "sampled_donors = donors.sample(n=num_samples, random_state=123)\n",
    "sampled_non_donors = non_donors.sample(n=num_samples, random_state=123)\n",
    "\n",
    "# Combine the sampled donors and non-donors\n",
    "sampled_data = pd.concat([sampled_donors, sampled_non_donors])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "sampled_data = sampled_data.sample(frac=1, random_state=123)\n",
    "\n",
    "# Split features and target variable\n",
    "X = sampled_data.drop(columns=['target'])\n",
    "y = sampled_data['target']\n",
    "\n",
    "# Perform one-hot encoding on categorical features in X\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Encode the target variable y\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=123)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier3 = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_classifier3 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, subsample=1.0, random_state=123)\n",
    "\n",
    "# Train the models\n",
    "rf_classifier3.fit(X_train, y_train)\n",
    "gb_classifier3.fit(X_train, y_train)\n",
    "\n",
    "# Predicted probabilities\n",
    "rf_probabilities3 = rf_classifier3.predict_proba(X_val)[:, 1]\n",
    "gb_probabilities3 = gb_classifier3.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Convert predictions back to original labels for evaluation metrics\n",
    "rf_predictions3_labels = le.inverse_transform(rf_classifier3.predict(X_val))\n",
    "gb_predictions3_labels = le.inverse_transform(gb_classifier3.predict(X_val))\n",
    "y_val_labels = le.inverse_transform(y_val)\n",
    "\n",
    "# Evaluation metrics\n",
    "def calculate_metrics(model_name, y_true, y_pred, y_pred_proba):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=\"Donor\")\n",
    "    recall = recall_score(y_true, y_pred, pos_label=\"Donor\")\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=\"Donor\")\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    return {'Model': model_name, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-score': f1, 'ROC AUC': roc_auc}\n",
    "\n",
    "# Calculate evaluation metrics for Random Forest Classifier\n",
    "rf_metrics = calculate_metrics('Random Forest Classifier', y_val_labels, rf_predictions3_labels, rf_probabilities3)\n",
    "\n",
    "# Calculate evaluation metrics for Gradient Boosting Classifier\n",
    "gb_metrics = calculate_metrics('Gradient Boosting Classifier', y_val_labels, gb_predictions3_labels, gb_probabilities3)\n",
    "\n",
    "# Display evaluation results\n",
    "evaluation_results_3 = pd.DataFrame([rf_metrics, gb_metrics])\n",
    "display(evaluation_results_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e882dab-2618-493b-b8f6-129742239307",
   "metadata": {},
   "source": [
    "### 4. Evaluate the fit. Examine the out of sample error for your models. Use tables or graphs to display your results. Is there a model that dominates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943fa87-bfea-4db8-9f15-26cfda713a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(evaluation_results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332fd24-0946-4665-b2d3-16c729551397",
   "metadata": {},
   "source": [
    "It looks like the Random Forest Classiuer is consistently better in our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622645d-0f79-450f-8365-4038226f24f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the model names as the index\n",
    "evaluation_results_2.set_index('Model', inplace=True)\n",
    "\n",
    "# Plot the performance metrics\n",
    "evaluation_results_2.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Performance Metrics of Random Forest vs. Gradient Boosting')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Model')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cf219-45b2-4d77-8d49-4f180c77c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Encode the target variable y_val\n",
    "label_encoder = LabelEncoder()\n",
    "y_val_encoded = label_encoder.fit_transform(y_val)\n",
    "\n",
    "# Predicted probabilities for Random Forest Classifier\n",
    "rf_probabilities = rf_classifier.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "# Predicted probabilities for Gradient Boosting Classifier\n",
    "gb_probabilities = gb_classifier.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "# Compute ROC curve for Random Forest Classifier\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_val_encoded, rf_probabilities, pos_label=label_encoder.transform(['Donor'])[0])\n",
    "\n",
    "# Compute ROC curve for Gradient Boosting Classifier\n",
    "gb_fpr, gb_tpr, _ = roc_curve(y_val_encoded, gb_probabilities, pos_label=label_encoder.transform(['Donor'])[0])\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(rf_fpr, rf_tpr, label='Random Forest')\n",
    "plt.plot(gb_fpr, gb_tpr, label='Gradient Boosting')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e45e5a-1201-486c-899a-2d7f7784c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Compute Precision-Recall curve for Random Forest Classifier\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_val_encoded, rf_probabilities, pos_label=label_encoder.transform(['Donor'])[0])\n",
    "\n",
    "# Compute Precision-Recall curve for Gradient Boosting Classifier\n",
    "gb_precision, gb_recall, _ = precision_recall_curve(y_val_encoded, gb_probabilities, pos_label=label_encoder.transform(['Donor'])[0])\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(rf_recall, rf_precision, label='Random Forest')\n",
    "plt.plot(gb_recall, gb_precision, label='Gradient Boosting')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b10ab06-c0a1-470d-86dc-2a1cf537deb3",
   "metadata": {},
   "source": [
    "### 5. Select best model. From your answer in (4), what do you think is the “best” model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f035e3-0c07-405c-a100-15a156fd5692",
   "metadata": {},
   "source": [
    "Based on these results, the Random Forest Classifier has slightly higher accuracy, precision, F1-score, and ROC AUC compared to the Gradient Boosting Classifier. However, the differences in performance metrics between the two models are relatively small.\n",
    "\n",
    "Therefore, the best model would likely depend on the specific priorities of the task. If we prioritize overall performance across multiple metrics, the Random Forest Classifier might be preferred. However, if interpretability or computational efficiency is a concern, the Gradient Boosting Classifier might be a better choice.\n",
    "\n",
    "Ultimately, it's essential to consider the trade-offs and requirements of the problem domain when selecting the best model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
